{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from typing import List, Callable, Union, Any, TypeVar, Tuple\n",
    "Tensor = TypeVar('torch.tensor')\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Data preprocessing utils : \n",
    "import torchio as tio\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as F\n",
    "import torch.nn.functional as f\n",
    "import torchvision.ops as ops\n",
    "from torchvision.transforms import Compose\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# Visuals utils\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# my defined model\n",
    "from vqVAE import VQVAE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/Users/kajou/OneDrive/Desktop/VQ-VAE/ACDC/database/training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following paramerters are set according to the VQ-VAE paper.\n",
    "\n",
    "L = 64 # image size L=W\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def get_max_BB(D3_slice):\n",
    "    masks = (D3_slice > 0).float()\n",
    "\n",
    "    # boxes = ops.masks_to_boxes(binary_masks)\n",
    "    boxes = []\n",
    "    for mask in masks:\n",
    "        # Get coordinates of non-zero pixels\n",
    "        y, x = torch.where(mask)\n",
    "        \n",
    "        if len(y) == 0:  # Empty mask\n",
    "            continue\n",
    "            \n",
    "        # Compute box coordinates\n",
    "        box = [\n",
    "            x.min(), y.min(),\n",
    "            x.max(), y.max()\n",
    "        ]\n",
    "        \n",
    "        boxes.append(box)\n",
    "        \n",
    "    boxes = torch.tensor(boxes)\n",
    "    largest_box = torch.tensor([\n",
    "        boxes[:, 0].min(),  # smallest x1\n",
    "        boxes[:, 1].min(),  # smallest y1\n",
    "        boxes[:, 2].max(),  # largest x2\n",
    "        boxes[:, 3].max()   # largest y2\n",
    "    ])\n",
    "    return largest_box\n",
    "\n",
    "def to_square_BB(BB):\n",
    "    min_col, min_row,max_col, max_row = BB\n",
    "    L = max_col - min_col\n",
    "    W = max_row - min_row\n",
    "    d = np.abs(L-W)\n",
    "    eps_left = d//2 \n",
    "    eps_right = d-eps_left\n",
    "    if L>W : \n",
    "        min_row -= eps_left\n",
    "        max_row += eps_right\n",
    "    else : \n",
    "        min_col -= eps_left\n",
    "        max_col += eps_right\n",
    "    return [min_col, min_row, max_col, max_row]\n",
    "\n",
    "\n",
    "def add_background_BB(suqare_BB,n_p = 10):\n",
    "    min_col, min_row, max_col, max_row = suqare_BB\n",
    "\n",
    "    min_col -= n_p\n",
    "    min_row -= n_p\n",
    "    max_row += n_p\n",
    "    max_col += n_p\n",
    "    BB_bg = [min_col, min_row, max_col, max_row]\n",
    "    return BB_bg\n",
    "\n",
    "def crop_img(D3_slice):\n",
    "    # BB = ops.masks_to_boxes(img.unsqueeze(0))[0].int().numpy()\n",
    "    BB = get_max_BB(D3_slice)\n",
    "    BB = to_square_BB(BB)\n",
    "    BB = add_background_BB(BB)\n",
    "    min_col, min_row,max_col, max_row = BB\n",
    "\n",
    "    img = D3_slice\n",
    "    croped = F.crop(img, min_row, min_col, max_row-min_row, max_col-min_col)\n",
    "    return croped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['patient001', 'patient002', 'patient003', 'patient004', 'patient005', 'patient006', 'patient007', 'patient008', 'patient009', 'patient010', 'patient011', 'patient012', 'patient013', 'patient014', 'patient015', 'patient016', 'patient017', 'patient018', 'patient019', 'patient020', 'patient021', 'patient022', 'patient023', 'patient024', 'patient025', 'patient026', 'patient027', 'patient028', 'patient029', 'patient030', 'patient031', 'patient032', 'patient033', 'patient034', 'patient035', 'patient036', 'patient037', 'patient038', 'patient039', 'patient040', 'patient041', 'patient042', 'patient043', 'patient044', 'patient045', 'patient046', 'patient047', 'patient048', 'patient049', 'patient050', 'patient051', 'patient052', 'patient053', 'patient054', 'patient055', 'patient056', 'patient057', 'patient058', 'patient059', 'patient060', 'patient061', 'patient062', 'patient063', 'patient064', 'patient065', 'patient066', 'patient067', 'patient068', 'patient069', 'patient070', 'patient071', 'patient072', 'patient073', 'patient074', 'patient075', 'patient076', 'patient077', 'patient078', 'patient079', 'patient080', 'patient081', 'patient082', 'patient083', 'patient084', 'patient085', 'patient086', 'patient087', 'patient088', 'patient089', 'patient090', 'patient091', 'patient092', 'patient093', 'patient094', 'patient095', 'patient096', 'patient097', 'patient098', 'patient099', 'patient100']\n",
      "frame12\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"/Users/kajou/OneDrive/Desktop/VQ-VAE/ACDC/database\"\n",
    "\n",
    "patients = os.listdir(os.path.join(dataset_path, \"training\"))[1:]\n",
    "print(patients)\n",
    "patient_PATH  = os.path.join(dataset_path, \"training\", \"patient002\")\n",
    "files = os.listdir(patient_PATH)\n",
    "image_files = [f for f in files if f.endswith('.nii.gz') and not f.endswith('_gt.nii.gz') and not f.endswith('4d.nii.gz')]\n",
    "frames = [f.split('_')[1].split('.')[0] for f in image_files ]\n",
    "\n",
    "print(max(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/Users/kajou/OneDrive/Desktop/VQ-VAE/ACDC/database\"\n",
    "\n",
    "def load_patient_gt(patient_PATH) :\n",
    "    files = os.listdir(patient_PATH)\n",
    "    image_files = [f for f in files if f.endswith('.nii.gz') and not f.endswith('_gt.nii.gz') and not f.endswith('4d.nii.gz')]\n",
    "    gt_files = [f for f in files if f.endswith('_gt.nii.gz')]\n",
    "\n",
    "\n",
    "    # Identify ED and ES frames\n",
    "    frames = [f.split('_')[1].split('.')[0] for f in image_files ]\n",
    "    ed_frame = min(frames)\n",
    "    es_frame = max(frames)\n",
    "\n",
    "    # Load ground truth segmentations\n",
    "    ed_gt = tio.LabelMap(os.path.join(patient_PATH, f\"patient{patient_PATH[-3:]}_{ed_frame}_gt.nii.gz\"))\n",
    "    es_gt = tio.LabelMap(os.path.join(patient_PATH, f\"patient{patient_PATH[-3:]}_{es_frame}_gt.nii.gz\"))\n",
    "    ed_gt_slices = crop_img(ed_gt.data.squeeze(0).permute(2,0,1)).flatten(start_dim = 0, end_dim=0) \n",
    "    es_gt_slices = crop_img(es_gt.data.squeeze(0).permute(2,0,1)).flatten(start_dim = 0, end_dim=0)\n",
    "\n",
    "\n",
    "    return [tensor for tensor in ed_gt_slices] + [tensor for tensor in es_gt_slices]\n",
    "\n",
    "def load_dataset(Path):\n",
    "    dataset = []\n",
    "    patients = os.listdir(Path)[1:]\n",
    "    for patient in patients : \n",
    "        patient_gt = load_patient_gt(os.path.join(Path, patient)) \n",
    "        dataset  += patient_gt\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_dataset = load_dataset(os.path.join(dataset_path, \"training\"))\n",
    "test_dataset = load_dataset(os.path.join(dataset_path, \"testing\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class One_hot_Transform:\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = x.squeeze(0).long()\n",
    "        one_hot_encoded = f.one_hot(x, num_classes=4)\n",
    "        return one_hot_encoded.permute(2, 0, 1)\n",
    "        \n",
    "input_transforms = Compose([\n",
    "    transforms.Resize(size=(L,L), interpolation=transforms.InterpolationMode.NEAREST),\n",
    "    One_hot_Transform(num_classes=4)\n",
    "    ])\n",
    "\n",
    "#define the dataset\n",
    "\n",
    "class ACDC_slices(Dataset):\n",
    "    def __init__(self, data, transforms =None):\n",
    "        self.data = data\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        item = item.unsqueeze(0)  # Shape: (1, H, W)\n",
    "\n",
    "        if self.transforms:\n",
    "            item = self.transforms(item)\n",
    "\n",
    "        return item\n",
    "    \n",
    "TrainDataset = ACDC_slices(data = train_dataset, transforms= input_transforms) \n",
    "TestDataset  = ACDC_slices(data = test_dataset, transforms= input_transforms)\n",
    "\n",
    "TrainLoader  = DataLoader(TrainDataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "TestLoader   = DataLoader(TestDataset , batch_size = BATCH_SIZE, shuffle = True)\n",
    "\n",
    "batch = next(iter(TrainLoader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepairing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "K =  512 # num_embeddings\n",
    "D =  64 # embedding_dim\n",
    "in_channels = 4 \n",
    "img_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACDC_VQVAE = VQVAE(in_channels, D, K)\n",
    "\n",
    "input = torch.rand(16, 4, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ACDC_VQVAE(input)\n",
    "z_e = ACDC_VQVAE.encode(input)[0]\n",
    "z_q, _ = ACDC_VQVAE.vq_layer(z_e)\n",
    "codeBook = ACDC_VQVAE.vq_layer.embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# detect gpu ?\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Learning parameters\n",
    "\n",
    "\n",
    "model = ACDC_VQVAE.to(device)\n",
    "batch_size = 64\n",
    "lr = 1e-4\n",
    "epochs = 2\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(batch.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [01:15<?, ?batch/s, loss=0.431]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss: 3.3519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [01:16<?, ?batch/s, loss=0.427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.4310\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "###########################        Training ....      #################################\n",
    "\n",
    "def evaluate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = batch.float().to(device)\n",
    "           \n",
    "            output, input, vq_loss = model(inputs)\n",
    "            \n",
    "            # Loss and backward\n",
    "            loss = model.loss_function(output, input, vq_loss)['loss']\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "    return avg_val_loss\n",
    "\n",
    "\n",
    "def save_model(model, epoch):\n",
    "    checkpoint_path = os.path.join( os.getcwd() , 'vqvae_100_bestmodel.pth' )\n",
    "    torch.save({'epoch' : epoch,\n",
    "                'model_state_dict': model.state_dict()}, checkpoint_path)\n",
    "\n",
    "\n",
    "\n",
    "model.train()\n",
    "train_loss_values = []\n",
    "val_loss_values = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    with tqdm(TrainLoader, unit=\"batch\") as tepoch:\n",
    "        for batch_idx, inputs in enumerate(TrainLoader):\n",
    "            inputs = inputs.float().to(device)  # Move data to the appropriate device (GPU/CPU)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass // args is a list containing : [output, input, vq_loss]\n",
    "            output, input, vq_loss = model(inputs)\n",
    "            \n",
    "            # Loss and backward\n",
    "            loss = model.loss_function(output, input, vq_loss)['loss']  # Use the loss function defined in the model\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track running loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # tqdm bar displays the loss\n",
    "            tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "    epoch_loss = train_loss / len(TrainLoader.dataset)\n",
    "    train_loss_values.append(epoch_loss)\n",
    "\n",
    "    # Validation after each epoch\n",
    "    val_loss = evaluate_model(model, TestLoader)\n",
    "    val_loss_values.append(val_loss)\n",
    "\n",
    "    #saving model if Loss values decreases\n",
    "    if val_loss < best_val_loss :\n",
    "        save_model(model, epoch)\n",
    "\n",
    "    print('Epoch {}: Train Loss: {:.4f}'.format(epoch, train_loss/len(TrainLoader)))\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
